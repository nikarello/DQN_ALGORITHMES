# algorithms/noisy_dueling_ddqn.py
import torch
import torch.optim as optim
from algorithms.base_trainer import BaseTrainer
from models.noisy_dueling_cnn import NoisyDuelingCNN
from core import device   # loss —É–∂–µ –µ—Å—Ç—å, –º–æ–∂–Ω–æ MSE
from core import ReplayBuffer                   # –æ–±—ã—á–Ω—ã–π –±—É—Ñ–µ—Ä

class NoisyDuelingDDQNTrainer(BaseTrainer):
    """Dueling Double DQN —Å factorised Noisy-Net (–±–µ–∑ Œµ)."""
    def __init__(self, envs, cfg):
        super().__init__(envs, cfg)

        self.online = NoisyDuelingCNN(self.in_ch, 4, self.view).to(device)
        self.target = NoisyDuelingCNN(self.in_ch, 4, self.view).to(device)
        self.target.load_state_dict(self.online.state_dict())
        self.target.eval()

        self.replay = ReplayBuffer(self.memory_size)
        self.optimizer = optim.Adam(self.online.parameters(), lr=self.lr)
        self.loss_fn   = torch.nn.MSELoss()

        self.epsilon = float("nan") 

    # ---------- –¥–µ–π—Å—Ç–≤–∏–π ----------
    def select_actions(self, views, mask):
        B, N, C, V, _ = views.shape
        flat = views.view(B * N, C, V, V)

        self.online.reset_noise()          # –∫–ª—é—á–µ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
        q = self.online(flat)
        acts = q.argmax(dim=1)
        return acts.view(B, N)

    # ---------- –æ–±—É—á–µ–Ω–∏–µ ----------
    def learn_step(self):
        if len(self.replay) < self.batch_size:
            return None

        S, A, R, NS, D = self.replay.sample(self.batch_size)

        # üé≤ –û–±–Ω–æ–≤–ª—è–µ–º —à—É–º (–¥–ª—è –æ–±–æ–∏—Ö —Å–µ—Ç–µ–π)
        self.online.reset_noise()
        self.target.reset_noise()

        # Q(s, a)
        q_curr = self.online(S).gather(1, A).squeeze()

        with torch.no_grad():
            best = self.online(NS).argmax(dim=1, keepdim=True)
            q_next = self.target(NS).gather(1, best).squeeze()
            tgt = R + self.gamma * q_next * (1 - D)

        loss = self.loss_fn(q_curr, tgt)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()


    # ---------- target sync ----------
    def after_episode(self, ep):
        if (ep + 1) % self.target_update_freq == 0:
            self.target.load_state_dict(self.online.state_dict())

    def build_model(self):
        return self.online, self.target
